# -*- coding: utf-8 -*-
"""output.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ACY3uHAsf4Ug3V6LEZFONJhikTeUaiEH
"""

print("output_classified_emails.csv")

print("output_classified_emails.csv")

print("output_classified_emails.csv")

!sudo apt install git-lfs
!pip install huggingface_hub

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

from huggingface_hub import notebook_login
notebook_login()

import os
import shutil

space_dir = "email_classifier_space"
os.makedirs(space_dir, exist_ok=True)

files_to_copy = [
    "api.py", "utils.py",
    "rf_classifier_v3.pkl",
    "vectorizer.pkl",
    "label_encoder.pkl"
]

for file in files_to_copy:
    shutil.copy(file, space_dir)

# Write requirements.txt
with open(f"{space_dir}/requirements.txt", "w") as f:
    f.write("""
fastapi
uvicorn
joblib
scikit-learn
pydantic
spacy
regex
imblearn
nltk
""")

# Write Dockerfile
with open(f"{space_dir}/Dockerfile", "w") as f:
    f.write("""
FROM python:3.10

WORKDIR /code

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
RUN python -m nltk.downloader stopwords wordnet

COPY . .

EXPOSE 7860
CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "7860"]
""")

# Commented out IPython magic to ensure Python compatibility.
from huggingface_hub import create_repo

space_id = "Nandini"  # customize this
create_repo(repo_id=space_id, repo_type="space", space_sdk="docker", exist_ok=True)

# Clone and push
!git clone https://huggingface.co/spaces/{space_id}
!cp -r {space_dir}/* email-classifier-space/
# %cd email-classifier-space
!git add .
!git commit -m "Initial commit from Colab"
!git push

!huggingface-cli login

!git lfs install
!git clone https://huggingface.co/spaces/nandini2455508/email-classifier-space

!cp api.py utils.py rf_classifier_v3.pkl vectorizer.pkl label_encoder.pkl email-classifier-space/

# Commented out IPython magic to ensure Python compatibility.
# %cd email-classifier-space

with open("email-classifier-space/Dockerfile", "w") as f:
    f.write('''
FROM python:3.9

WORKDIR /app

COPY . .

RUN pip install --no-cache-dir fastapi uvicorn scikit-learn pandas numpy joblib imbalanced-learn nltk

RUN python -m nltk.downloader stopwords wordnet

EXPOSE 7860

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "7860"]
''')

!git clone https://huggingface.co/spaces/nandini2455508/email-classifier-space

# Commented out IPython magic to ensure Python compatibility.
# %%writefile email-classifier-space/Dockerfile
# FROM python:3.9
# 
# WORKDIR /app
# 
# COPY requirements.txt .
# RUN pip install -r requirements.txt
# 
# COPY . .
# 
# CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "7860"]
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile email-classifier-space/requirements.txt
# fastapi
# uvicorn
# joblib
# scikit-learn
# pandas
# numpy
# nltk
# spacy
# imblearn
#



!mv /content/api.py /content/utils.py /content/vectorizer.pkl /content/label_encoder.pkl /content/rf_classifier_v3.pkl /content/email_classifier_space

!ls email-classifier-space

!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd email-classifier-space
!git lfs install
!git add .
!git commit -m "Initial deploy"
!git push

!git config --global user.email "pathivadanandini24@gmail.com!"

!git config --global user.name "nandini2455508"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/email-classifier-space/
!git lfs install
!git add .
!git commit -m "Initial deploy"
!git push

# Commented out IPython magic to ensure Python compatibility.
# %cd email-classifier-space

# Pull the latest changes from Hugging Face
!git pull origin main --rebase

# Add your files (if not already added)
!git add .

# Commit your changes
!git commit -m "Added email classifier API files"

# Push to Hugging Face
!git push origin main

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/email-classifier-space

import pandas as pd
import joblib
from utils import mask_pii
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
import re

# Load models
model = joblib.load("/content/rf_classifier_v3.pkl")
vectorizer = joblib.load("/content/vectorizer.pkl")
label_encoder = joblib.load("/content/label_encoder.pkl")

# NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Text cleaning function
def clean_text(text):
    text = re.sub(r'\n|\r', ' ', text)
    text = re.sub(r'Subject:', '', text)
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Load your dataset
df = pd.read_csv("/content/combined_emails_with_natural_pii.csv")  # Make sure this file has 'email' and 'type' columns

# Output list to store processed data
results = []

for idx, row in df.iterrows():
    original_email = row['email']
    actual_type = row['type']
    masked_text, pii_entities = mask_pii(original_email)
    cleaned = clean_text(masked_text)
    vec = vectorizer.transform([cleaned])
    pred = model.predict(vec)
    predicted_category = label_encoder.inverse_transform(pred)[0]

    results.append({
        "original_email": original_email,
        "masked_email": masked_text,
        "actual_type": actual_type,
        "predicted_category": predicted_category,
        "pii_entities": pii_entities
    })

# Convert to DataFrame and save
output_df = pd.DataFrame(results)
output_df.to_csv("output_classified_emails.csv", index=False)
print("âœ… Results saved to output_classified_emails.csv")

print("output_classified_emails.csv")

from google.colab import files

files.download('output_classified_emails.csv')

